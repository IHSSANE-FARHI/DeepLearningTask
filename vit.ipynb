{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1915 images belonging to 7 classes.\n",
      "Found 477 images belonging to 7 classes.\n",
      "Found 70 images belonging to 7 classes.\n",
      "\n",
      "Classes détectées (train) : {'Bear': 0, 'Camel': 1, 'Chiken': 2, 'Elephent': 3, 'Horse': 4, 'Lion': 5, 'Squirrel': 6}\n",
      "Nombre de classes (train) : 7\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 256)          196864    ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)         (None, 196, 256)             0         ['conv2d_1[0][0]']            \n",
      "                                                                                                  \n",
      " cls_token_layer_1 (CLSToke  (None, 197, 256)             256       ['reshape_1[0][0]']           \n",
      " nLayer)                                                                                          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_13 (T  (None, 197, 256)             0         ['cls_token_layer_1[0][0]']   \n",
      " FOpLambda)                                                                                       \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)         (None, 197, 256)             0         ['tf.__operators__.add_13[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_13 (La  (None, 197, 256)             512       ['dropout_7[0][0]']           \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " multi_head_attention_6 (Mu  (None, 197, 256)             263168    ['layer_normalization_13[0][0]\n",
      " ltiHeadAttention)                                                  ',                            \n",
      "                                                                     'layer_normalization_13[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_14 (T  (None, 197, 256)             0         ['dropout_7[0][0]',           \n",
      " FOpLambda)                                                          'multi_head_attention_6[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_14 (La  (None, 197, 256)             512       ['tf.__operators__.add_14[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " dense_13 (Dense)            (None, 197, 512)             131584    ['layer_normalization_14[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)         (None, 197, 512)             0         ['dense_13[0][0]']            \n",
      "                                                                                                  \n",
      " dense_14 (Dense)            (None, 197, 256)             131328    ['dropout_8[0][0]']           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_15 (T  (None, 197, 256)             0         ['tf.__operators__.add_14[0][0\n",
      " FOpLambda)                                                         ]',                           \n",
      "                                                                     'dense_14[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_15 (La  (None, 197, 256)             512       ['tf.__operators__.add_15[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_7 (Mu  (None, 197, 256)             263168    ['layer_normalization_15[0][0]\n",
      " ltiHeadAttention)                                                  ',                            \n",
      "                                                                     'layer_normalization_15[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_16 (T  (None, 197, 256)             0         ['tf.__operators__.add_15[0][0\n",
      " FOpLambda)                                                         ]',                           \n",
      "                                                                     'multi_head_attention_7[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_16 (La  (None, 197, 256)             512       ['tf.__operators__.add_16[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " dense_15 (Dense)            (None, 197, 512)             131584    ['layer_normalization_16[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)         (None, 197, 512)             0         ['dense_15[0][0]']            \n",
      "                                                                                                  \n",
      " dense_16 (Dense)            (None, 197, 256)             131328    ['dropout_9[0][0]']           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_17 (T  (None, 197, 256)             0         ['tf.__operators__.add_16[0][0\n",
      " FOpLambda)                                                         ]',                           \n",
      "                                                                     'dense_16[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_17 (La  (None, 197, 256)             512       ['tf.__operators__.add_17[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_8 (Mu  (None, 197, 256)             263168    ['layer_normalization_17[0][0]\n",
      " ltiHeadAttention)                                                  ',                            \n",
      "                                                                     'layer_normalization_17[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_18 (T  (None, 197, 256)             0         ['tf.__operators__.add_17[0][0\n",
      " FOpLambda)                                                         ]',                           \n",
      "                                                                     'multi_head_attention_8[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_18 (La  (None, 197, 256)             512       ['tf.__operators__.add_18[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " dense_17 (Dense)            (None, 197, 512)             131584    ['layer_normalization_18[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)        (None, 197, 512)             0         ['dense_17[0][0]']            \n",
      "                                                                                                  \n",
      " dense_18 (Dense)            (None, 197, 256)             131328    ['dropout_10[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_19 (T  (None, 197, 256)             0         ['tf.__operators__.add_18[0][0\n",
      " FOpLambda)                                                         ]',                           \n",
      "                                                                     'dense_18[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_19 (La  (None, 197, 256)             512       ['tf.__operators__.add_19[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_9 (Mu  (None, 197, 256)             263168    ['layer_normalization_19[0][0]\n",
      " ltiHeadAttention)                                                  ',                            \n",
      "                                                                     'layer_normalization_19[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_20 (T  (None, 197, 256)             0         ['tf.__operators__.add_19[0][0\n",
      " FOpLambda)                                                         ]',                           \n",
      "                                                                     'multi_head_attention_9[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_20 (La  (None, 197, 256)             512       ['tf.__operators__.add_20[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " dense_19 (Dense)            (None, 197, 512)             131584    ['layer_normalization_20[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)        (None, 197, 512)             0         ['dense_19[0][0]']            \n",
      "                                                                                                  \n",
      " dense_20 (Dense)            (None, 197, 256)             131328    ['dropout_11[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_21 (T  (None, 197, 256)             0         ['tf.__operators__.add_20[0][0\n",
      " FOpLambda)                                                         ]',                           \n",
      "                                                                     'dense_20[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_21 (La  (None, 197, 256)             512       ['tf.__operators__.add_21[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_10 (M  (None, 197, 256)             263168    ['layer_normalization_21[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_21[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_22 (T  (None, 197, 256)             0         ['tf.__operators__.add_21[0][0\n",
      " FOpLambda)                                                         ]',                           \n",
      "                                                                     'multi_head_attention_10[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_22 (La  (None, 197, 256)             512       ['tf.__operators__.add_22[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " dense_21 (Dense)            (None, 197, 512)             131584    ['layer_normalization_22[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)        (None, 197, 512)             0         ['dense_21[0][0]']            \n",
      "                                                                                                  \n",
      " dense_22 (Dense)            (None, 197, 256)             131328    ['dropout_12[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_23 (T  (None, 197, 256)             0         ['tf.__operators__.add_22[0][0\n",
      " FOpLambda)                                                         ]',                           \n",
      "                                                                     'dense_22[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_23 (La  (None, 197, 256)             512       ['tf.__operators__.add_23[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " multi_head_attention_11 (M  (None, 197, 256)             263168    ['layer_normalization_23[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'layer_normalization_23[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_24 (T  (None, 197, 256)             0         ['tf.__operators__.add_23[0][0\n",
      " FOpLambda)                                                         ]',                           \n",
      "                                                                     'multi_head_attention_11[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_24 (La  (None, 197, 256)             512       ['tf.__operators__.add_24[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " dense_23 (Dense)            (None, 197, 512)             131584    ['layer_normalization_24[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)        (None, 197, 512)             0         ['dense_23[0][0]']            \n",
      "                                                                                                  \n",
      " dense_24 (Dense)            (None, 197, 256)             131328    ['dropout_13[0][0]']          \n",
      "                                                                                                  \n",
      " tf.__operators__.add_25 (T  (None, 197, 256)             0         ['tf.__operators__.add_24[0][0\n",
      " FOpLambda)                                                         ]',                           \n",
      "                                                                     'dense_24[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_25 (La  (None, 197, 256)             512       ['tf.__operators__.add_25[0][0\n",
      " yerNormalization)                                                  ]']                           \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)           (None, 256)                  0         ['layer_normalization_25[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_25 (Dense)            (None, 7)                    1799      ['lambda_1[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3362055 (12.83 MB)\n",
      "Trainable params: 3362055 (12.83 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "CPU: 0.0% | RAM: 93.6%\n",
      "Epoch 1/40\n",
      "26/60 [============>.................] - ETA: 59s - loss: 0.1254 - accuracy: 0.1490 CPU: 35.2% | RAM: 89.7%\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.1130 - accuracy: 0.1885CPU: 39.3% | RAM: 87.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Probook\\OneDrive\\Bureau\\DeepLearningTask\\myenv\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 128s 2s/step - loss: 0.1130 - accuracy: 0.1885 - val_loss: 1.9059 - val_accuracy: 0.2117\n",
      "Epoch 2/40\n",
      "31/60 [==============>...............] - ETA: 46s - loss: 0.1033 - accuracy: 0.2158CPU: 30.1% | RAM: 87.1%\n",
      "60/60 [==============================] - 108s 2s/step - loss: 0.1020 - accuracy: 0.2219 - val_loss: 1.8539 - val_accuracy: 0.2285\n",
      "Epoch 3/40\n",
      " 1/60 [..............................] - ETA: 1:49 - loss: 0.0944 - accuracy: 0.3438CPU: 28.2% | RAM: 87.6%\n",
      "37/60 [=================>............] - ETA: 38s - loss: 0.0981 - accuracy: 0.2477CPU: 39.9% | RAM: 88.3%\n",
      "60/60 [==============================] - 111s 2s/step - loss: 0.0980 - accuracy: 0.2491 - val_loss: 1.7946 - val_accuracy: 0.2516\n",
      "Epoch 4/40\n",
      " 7/60 [==>...........................] - ETA: 1:24 - loss: 0.0950 - accuracy: 0.2694CPU: 33.3% | RAM: 85.9%\n",
      "43/60 [====================>.........] - ETA: 27s - loss: 0.0970 - accuracy: 0.2721CPU: 34.0% | RAM: 85.1%\n",
      "60/60 [==============================] - 109s 2s/step - loss: 0.0961 - accuracy: 0.2794 - val_loss: 1.7927 - val_accuracy: 0.2537\n",
      "Epoch 5/40\n",
      "13/60 [=====>........................] - ETA: 1:16 - loss: 0.0942 - accuracy: 0.2644CPU: 29.1% | RAM: 85.6%\n",
      "50/60 [========================>.....] - ETA: 16s - loss: 0.0967 - accuracy: 0.2715CPU: 33.7% | RAM: 85.6%\n",
      "60/60 [==============================] - 110s 2s/step - loss: 0.0962 - accuracy: 0.2757 - val_loss: 1.9923 - val_accuracy: 0.2725\n",
      "Epoch 6/40\n",
      "17/60 [=======>......................] - ETA: 1:23 - loss: 0.0942 - accuracy: 0.3051CPU: 42.7% | RAM: 89.5%\n",
      "49/60 [=======================>......] - ETA: 20s - loss: 0.0940 - accuracy: 0.3084CPU: 48.6% | RAM: 86.1%\n",
      "60/60 [==============================] - 127s 2s/step - loss: 0.0935 - accuracy: 0.3081 - val_loss: 1.7514 - val_accuracy: 0.3270\n",
      "Epoch 7/40\n",
      "14/60 [======>.......................] - ETA: 1:22 - loss: 0.0923 - accuracy: 0.3058CPU: 34.4% | RAM: 85.5%\n",
      "47/60 [======================>.......] - ETA: 23s - loss: 0.0900 - accuracy: 0.3482CPU: 46.3% | RAM: 85.2%\n",
      "60/60 [==============================] - 122s 2s/step - loss: 0.0892 - accuracy: 0.3608 - val_loss: 1.8667 - val_accuracy: 0.2998\n",
      "Epoch 8/40\n",
      "12/60 [=====>........................] - ETA: 1:28 - loss: 0.0892 - accuracy: 0.3646CPU: 35.7% | RAM: 85.3%\n",
      "45/60 [=====================>........] - ETA: 27s - loss: 0.0866 - accuracy: 0.3777CPU: 43.7% | RAM: 85.0%\n",
      "60/60 [==============================] - 124s 2s/step - loss: 0.0867 - accuracy: 0.3775 - val_loss: 1.7691 - val_accuracy: 0.2704\n",
      "Epoch 9/40\n",
      "10/60 [====>.........................] - ETA: 1:31 - loss: 0.0828 - accuracy: 0.3969CPU: 37.1% | RAM: 84.1%\n",
      "43/60 [====================>.........] - ETA: 31s - loss: 0.0836 - accuracy: 0.4055CPU: 40.5% | RAM: 84.4%\n",
      "60/60 [==============================] - 125s 2s/step - loss: 0.0821 - accuracy: 0.4245 - val_loss: 1.6277 - val_accuracy: 0.3962\n",
      "Epoch 10/40\n",
      " 8/60 [===>..........................] - ETA: 1:31 - loss: 0.0741 - accuracy: 0.4900CPU: 34.3% | RAM: 84.3%\n",
      "40/60 [===================>..........] - ETA: 36s - loss: 0.0795 - accuracy: 0.4431CPU: 38.3% | RAM: 84.6%\n",
      "60/60 [==============================] - 124s 2s/step - loss: 0.0790 - accuracy: 0.4465 - val_loss: 1.7245 - val_accuracy: 0.3669\n",
      "Epoch 11/40\n",
      " 5/60 [=>............................] - ETA: 1:37 - loss: 0.0829 - accuracy: 0.4387CPU: 35.7% | RAM: 85.1%\n",
      "37/60 [=================>............] - ETA: 42s - loss: 0.0782 - accuracy: 0.4741CPU: 43.6% | RAM: 83.3%\n",
      "60/60 [==============================] - 126s 2s/step - loss: 0.0779 - accuracy: 0.4705 - val_loss: 1.6607 - val_accuracy: 0.3857\n",
      "Epoch 12/40\n",
      " 2/60 [>.............................] - ETA: 1:48 - loss: 0.0733 - accuracy: 0.4531CPU: 39.0% | RAM: 83.3%\n",
      "34/60 [================>.............] - ETA: 48s - loss: 0.0780 - accuracy: 0.4497CPU: 43.6% | RAM: 84.5%\n",
      "60/60 [==============================] - 120s 2s/step - loss: 0.0777 - accuracy: 0.4658 - val_loss: 1.5690 - val_accuracy: 0.4235\n",
      "Epoch 13/40\n",
      " 2/60 [>.............................] - ETA: 1:36 - loss: 0.0757 - accuracy: 0.4219CPU: 30.6% | RAM: 84.1%\n",
      "38/60 [==================>...........] - ETA: 36s - loss: 0.0769 - accuracy: 0.4855CPU: 38.8% | RAM: 84.0%\n",
      "60/60 [==============================] - 111s 2s/step - loss: 0.0760 - accuracy: 0.4846 - val_loss: 1.5132 - val_accuracy: 0.4151\n",
      "Epoch 14/40\n",
      " 8/60 [===>..........................] - ETA: 1:28 - loss: 0.0666 - accuracy: 0.5625CPU: 29.6% | RAM: 81.4%\n",
      "41/60 [===================>..........] - ETA: 34s - loss: 0.0738 - accuracy: 0.5065CPU: 50.1% | RAM: 84.7%\n",
      "60/60 [==============================] - 115s 2s/step - loss: 0.0742 - accuracy: 0.5008 - val_loss: 1.5471 - val_accuracy: 0.4109\n",
      "Epoch 15/40\n",
      "11/60 [====>.........................] - ETA: 1:19 - loss: 0.0714 - accuracy: 0.5114CPU: 29.7% | RAM: 85.0%\n",
      "47/60 [======================>.......] - ETA: 21s - loss: 0.0725 - accuracy: 0.5137CPU: 31.4% | RAM: 84.7%\n",
      "60/60 [==============================] - 109s 2s/step - loss: 0.0720 - accuracy: 0.5196 - val_loss: 1.5513 - val_accuracy: 0.4067\n",
      "Epoch 16/40\n",
      "18/60 [========>.....................] - ETA: 1:07 - loss: 0.0739 - accuracy: 0.4869CPU: 25.5% | RAM: 84.1%\n",
      "55/60 [==========================>...] - ETA: 8s - loss: 0.0715 - accuracy: 0.5140CPU: 29.7% | RAM: 82.0%\n",
      "60/60 [==============================] - 110s 2s/step - loss: 0.0717 - accuracy: 0.5154 - val_loss: 1.4982 - val_accuracy: 0.4444\n",
      "Epoch 17/40\n",
      "23/60 [==========>...................] - ETA: 1:01 - loss: 0.0759 - accuracy: 0.4692CPU: 31.5% | RAM: 78.4%\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.0721 - accuracy: 0.5034CPU: 24.8% | RAM: 77.4%\n",
      "60/60 [==============================] - 109s 2s/step - loss: 0.0721 - accuracy: 0.5034 - val_loss: 1.4876 - val_accuracy: 0.4382\n",
      "Epoch 18/40\n",
      "30/60 [==============>...............] - ETA: 49s - loss: 0.0701 - accuracy: 0.5302CPU: 27.6% | RAM: 78.3%\n",
      "60/60 [==============================] - 109s 2s/step - loss: 0.0698 - accuracy: 0.5258 - val_loss: 1.5008 - val_accuracy: 0.4423\n",
      "Epoch 19/40\n",
      " 1/60 [..............................] - ETA: 1:51 - loss: 0.0610 - accuracy: 0.4688CPU: 26.4% | RAM: 78.1%\n",
      "37/60 [=================>............] - ETA: 37s - loss: 0.0690 - accuracy: 0.5177CPU: 30.5% | RAM: 77.4%\n",
      "60/60 [==============================] - 109s 2s/step - loss: 0.0689 - accuracy: 0.5311 - val_loss: 1.5057 - val_accuracy: 0.4675\n",
      "Epoch 20/40\n",
      " 7/60 [==>...........................] - ETA: 1:28 - loss: 0.0710 - accuracy: 0.5134CPU: 28.7% | RAM: 77.7%\n",
      "44/60 [=====================>........] - ETA: 26s - loss: 0.0679 - accuracy: 0.5324CPU: 30.0% | RAM: 79.1%\n",
      "60/60 [==============================] - 110s 2s/step - loss: 0.0667 - accuracy: 0.5488 - val_loss: 1.4489 - val_accuracy: 0.4906\n",
      "Epoch 21/40\n",
      "14/60 [======>.......................] - ETA: 1:14 - loss: 0.0655 - accuracy: 0.5379CPU: 30.5% | RAM: 76.7%\n",
      "48/60 [=======================>......] - ETA: 20s - loss: 0.0671 - accuracy: 0.5369CPU: 44.4% | RAM: 79.3%\n",
      "60/60 [==============================] - 113s 2s/step - loss: 0.0669 - accuracy: 0.5426 - val_loss: 1.4840 - val_accuracy: 0.4696\n",
      "Epoch 22/40\n",
      "16/60 [=======>......................] - ETA: 1:19 - loss: 0.0697 - accuracy: 0.5365CPU: 35.3% | RAM: 77.1%\n",
      "49/60 [=======================>......] - ETA: 19s - loss: 0.0674 - accuracy: 0.5521CPU: 31.4% | RAM: 75.8%\n",
      "60/60 [==============================] - 121s 2s/step - loss: 0.0677 - accuracy: 0.5488 - val_loss: 1.5306 - val_accuracy: 0.4654\n",
      "Epoch 23/40\n",
      "17/60 [=======>......................] - ETA: 1:10 - loss: 0.0614 - accuracy: 0.5974CPU: 24.8% | RAM: 75.9%\n",
      "54/60 [==========================>...] - ETA: 9s - loss: 0.0657 - accuracy: 0.5613 CPU: 29.8% | RAM: 76.0%\n",
      "60/60 [==============================] - 109s 2s/step - loss: 0.0661 - accuracy: 0.5577 - val_loss: 1.4388 - val_accuracy: 0.4990\n",
      "Epoch 24/40\n",
      "24/60 [===========>..................] - ETA: 58s - loss: 0.0642 - accuracy: 0.5701 CPU: 29.8% | RAM: 76.5%\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.0642 - accuracy: 0.5681CPU: 26.6% | RAM: 75.6%\n",
      "60/60 [==============================] - 108s 2s/step - loss: 0.0642 - accuracy: 0.5681 - val_loss: 1.4066 - val_accuracy: 0.4801\n",
      "Epoch 25/40\n",
      "30/60 [==============>...............] - ETA: 51s - loss: 0.0646 - accuracy: 0.5539CPU: 41.1% | RAM: 75.5%\n",
      "60/60 [==============================] - 111s 2s/step - loss: 0.0638 - accuracy: 0.5603 - val_loss: 1.3586 - val_accuracy: 0.5199\n",
      "Epoch 26/40\n",
      "CPU: 27.8% | RAM: 75.5%\n",
      "36/60 [=================>............] - ETA: 39s - loss: 0.0645 - accuracy: 0.5850CPU: 31.2% | RAM: 75.0%\n",
      "60/60 [==============================] - 109s 2s/step - loss: 0.0635 - accuracy: 0.5875 - val_loss: 1.4126 - val_accuracy: 0.4822\n",
      "Epoch 27/40\n",
      " 6/60 [==>...........................] - ETA: 1:26 - loss: 0.0580 - accuracy: 0.6198CPU: 26.6% | RAM: 75.0%\n",
      "44/60 [=====================>........] - ETA: 25s - loss: 0.0625 - accuracy: 0.5745CPU: 28.0% | RAM: 74.8%\n",
      "60/60 [==============================] - 109s 2s/step - loss: 0.0625 - accuracy: 0.5791 - val_loss: 1.4226 - val_accuracy: 0.5031\n",
      "Epoch 28/40\n",
      "12/60 [=====>........................] - ETA: 1:23 - loss: 0.0619 - accuracy: 0.5990CPU: 38.6% | RAM: 75.4%\n",
      "48/60 [=======================>......] - ETA: 20s - loss: 0.0625 - accuracy: 0.5800CPU: 38.7% | RAM: 75.5%\n",
      "60/60 [==============================] - 111s 2s/step - loss: 0.0625 - accuracy: 0.5786 - val_loss: 1.4514 - val_accuracy: 0.4717\n",
      "Epoch 29/40\n",
      "19/60 [========>.....................] - ETA: 1:06 - loss: 0.0602 - accuracy: 0.5938CPU: 25.6% | RAM: 74.6%\n",
      "53/60 [=========================>....] - ETA: 12s - loss: 0.0608 - accuracy: 0.5973CPU: 47.9% | RAM: 72.1%\n",
      "60/60 [==============================] - 118s 2s/step - loss: 0.0608 - accuracy: 0.5979 - val_loss: 1.4028 - val_accuracy: 0.5052\n",
      "Epoch 30/40\n",
      "CPU: 10.6% | RAM: 72.3%\n",
      "38/60 [==================>...........] - ETA: 34s - loss: 0.0598 - accuracy: 0.5879CPU: 33.1% | RAM: 72.3%\n",
      "60/60 [==============================] - 164s 2s/step - loss: 0.0606 - accuracy: 0.5770 - val_loss: 1.3819 - val_accuracy: 0.4990\n",
      "Epoch 31/40\n",
      " 9/60 [===>..........................] - ETA: 1:20 - loss: 0.0574 - accuracy: 0.5868CPU: 28.2% | RAM: 73.0%\n",
      "44/60 [=====================>........] - ETA: 26s - loss: 0.0588 - accuracy: 0.5880CPU: 43.0% | RAM: 74.2%\n",
      "60/60 [==============================] - 113s 2s/step - loss: 0.0598 - accuracy: 0.5875 - val_loss: 1.3901 - val_accuracy: 0.5073\n",
      "Epoch 32/40\n",
      "14/60 [======>.......................] - ETA: 1:13 - loss: 0.0597 - accuracy: 0.6049CPU: 38.6% | RAM: 75.0%\n",
      "42/60 [====================>.........] - ETA: 34s - loss: 0.0591 - accuracy: 0.6057CPU: 66.7% | RAM: 78.6%\n",
      "60/60 [==============================] - 130s 2s/step - loss: 0.0590 - accuracy: 0.6099 - val_loss: 1.3931 - val_accuracy: 0.5220\n",
      "Epoch 33/40\n",
      " 6/60 [==>...........................] - ETA: 1:40 - loss: 0.0569 - accuracy: 0.6458CPU: 47.3% | RAM: 77.8%\n",
      "39/60 [==================>...........] - ETA: 38s - loss: 0.0577 - accuracy: 0.6250CPU: 55.8% | RAM: 77.2%\n",
      "60/60 [==============================] - 124s 2s/step - loss: 0.0588 - accuracy: 0.6078 - val_loss: 1.3904 - val_accuracy: 0.5199\n",
      "Epoch 34/40\n",
      " 4/60 [=>............................] - ETA: 1:41 - loss: 0.0557 - accuracy: 0.6406CPU: 45.2% | RAM: 76.7%\n",
      "37/60 [=================>............] - ETA: 41s - loss: 0.0582 - accuracy: 0.6140CPU: 54.3% | RAM: 77.0%\n",
      "60/60 [==============================] - 123s 2s/step - loss: 0.0587 - accuracy: 0.6094 - val_loss: 1.3785 - val_accuracy: 0.5325\n",
      "Epoch 35/40\n",
      " 2/60 [>.............................] - ETA: 1:46 - loss: 0.0593 - accuracy: 0.5781CPU: 41.8% | RAM: 77.5%\n",
      "34/60 [================>.............] - ETA: 48s - loss: 0.0589 - accuracy: 0.6241CPU: 55.4% | RAM: 77.0%\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.0577 - accuracy: 0.6240CPU: 44.0% | RAM: 77.7%\n",
      "60/60 [==============================] - 127s 2s/step - loss: 0.0577 - accuracy: 0.6240 - val_loss: 1.3489 - val_accuracy: 0.5472\n",
      "Epoch 36/40\n",
      "30/60 [==============>...............] - ETA: 57s - loss: 0.0600 - accuracy: 0.5938CPU: 56.6% | RAM: 79.0%\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.0580 - accuracy: 0.6068CPU: 52.9% | RAM: 79.1%\n",
      "60/60 [==============================] - 129s 2s/step - loss: 0.0580 - accuracy: 0.6068 - val_loss: 1.3358 - val_accuracy: 0.5283\n",
      "Epoch 37/40\n",
      "26/60 [============>.................] - ETA: 1:02 - loss: 0.0598 - accuracy: 0.6034CPU: 47.9% | RAM: 79.2%\n",
      "58/60 [============================>.] - ETA: 3s - loss: 0.0576 - accuracy: 0.6283CPU: 55.5% | RAM: 79.0%\n",
      "60/60 [==============================] - 127s 2s/step - loss: 0.0576 - accuracy: 0.6261 - val_loss: 1.4113 - val_accuracy: 0.5115\n",
      "Epoch 38/40\n",
      "22/60 [==========>...................] - ETA: 1:11 - loss: 0.0584 - accuracy: 0.5994CPU: 47.7% | RAM: 80.1%\n",
      "54/60 [==========================>...] - ETA: 11s - loss: 0.0576 - accuracy: 0.6198CPU: 57.5% | RAM: 79.6%\n",
      "60/60 [==============================] - 128s 2s/step - loss: 0.0576 - accuracy: 0.6188 - val_loss: 1.3660 - val_accuracy: 0.5157\n",
      "Epoch 39/40\n",
      "18/60 [========>.....................] - ETA: 1:18 - loss: 0.0561 - accuracy: 0.6319CPU: 43.6% | RAM: 80.7%\n",
      "50/60 [========================>.....] - ETA: 18s - loss: 0.0563 - accuracy: 0.6307CPU: 51.1% | RAM: 79.7%\n",
      "60/60 [==============================] - 128s 2s/step - loss: 0.0565 - accuracy: 0.6303 - val_loss: 1.3293 - val_accuracy: 0.5220\n",
      "Epoch 40/40\n",
      "14/60 [======>.......................] - ETA: 1:25 - loss: 0.0549 - accuracy: 0.6161CPU: 45.6% | RAM: 80.3%\n",
      "46/60 [======================>.......] - ETA: 26s - loss: 0.0563 - accuracy: 0.6217CPU: 54.5% | RAM: 80.5%\n",
      "60/60 [==============================] - 128s 2s/step - loss: 0.0563 - accuracy: 0.6204 - val_loss: 1.4432 - val_accuracy: 0.4927\n",
      "\n",
      "Meilleure val_accuracy pendant l'entraînement : 54.72%\n",
      "\n",
      "Chargement des meilleurs poids (best_vit.h5) ...\n",
      "\n",
      "Évaluation finale sur la Validation :\n",
      "15/15 [==============================] - 14s 944ms/step - loss: 1.3672 - accuracy: 0.5241\n",
      "Val Loss: 1.3672 | Val Accuracy: 52.41%\n",
      "\n",
      "Évaluation finale sur le Test :\n",
      "3/3 [==============================] - 1s 414ms/step - loss: 0.9593 - accuracy: 0.6429\n",
      "Test Loss: 0.9593 | Test Accuracy: 64.29%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exemples de prédictions sur la Validation :\n",
      "CPU: 35.1% | RAM: 81.3%\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "Prédiction: Bear | Vérité: Bear\n",
      "Prédiction: Elephent | Vérité: Bear\n",
      "Prédiction: Bear | Vérité: Bear\n",
      "\n",
      "Exemples de prédictions sur le Test :\n",
      "1/1 [==============================] - 0s 469ms/step\n",
      "Prédiction: BearTest | Vérité: BearTest\n",
      "Prédiction: BearTest | Vérité: BearTest\n",
      "Prédiction: BearTest | Vérité: BearTest\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: 3.5% | RAM: 80.5%\n",
      "CPU: 5.2% | RAM: 83.3%\n",
      "CPU: 8.2% | RAM: 83.2%\n",
      "CPU: 9.6% | RAM: 82.8%\n",
      "CPU: 4.8% | RAM: 82.6%\n",
      "CPU: 4.1% | RAM: 86.1%\n",
      "CPU: 15.4% | RAM: 95.3%\n",
      "CPU: 30.0% | RAM: 95.1%\n",
      "CPU: 3.8% | RAM: 94.2%\n",
      "CPU: 1.4% | RAM: 93.9%\n",
      "CPU: 15.0% | RAM: 91.7%\n",
      "CPU: 4.4% | RAM: 90.7%\n",
      "CPU: 3.9% | RAM: 90.0%\n",
      "CPU: 3.6% | RAM: 90.0%\n",
      "CPU: 3.1% | RAM: 89.7%\n",
      "CPU: 3.5% | RAM: 89.5%\n",
      "CPU: 7.2% | RAM: 91.1%\n",
      "CPU: 15.9% | RAM: 92.8%\n",
      "CPU: 8.7% | RAM: 90.0%\n",
      "CPU: 14.7% | RAM: 90.9%\n",
      "CPU: 5.5% | RAM: 92.2%\n",
      "CPU: 22.5% | RAM: 81.6%\n",
      "CPU: 15.6% | RAM: 85.8%\n",
      "CPU: 16.1% | RAM: 84.6%\n",
      "CPU: 49.9% | RAM: 86.5%\n",
      "CPU: 57.1% | RAM: 85.9%\n",
      "CPU: 57.6% | RAM: 83.3%\n",
      "CPU: 57.6% | RAM: 84.6%\n",
      "CPU: 46.5% | RAM: 83.7%\n",
      "CPU: 34.9% | RAM: 83.7%\n",
      "CPU: 46.8% | RAM: 84.5%\n",
      "CPU: 51.1% | RAM: 83.5%\n",
      "CPU: 47.9% | RAM: 84.8%\n",
      "CPU: 55.5% | RAM: 84.0%\n",
      "CPU: 54.6% | RAM: 83.8%\n",
      "CPU: 44.7% | RAM: 86.0%\n",
      "CPU: 68.9% | RAM: 83.1%\n",
      "CPU: 48.8% | RAM: 83.0%\n",
      "CPU: 52.5% | RAM: 85.2%\n",
      "CPU: 56.1% | RAM: 83.9%\n",
      "CPU: 52.4% | RAM: 83.5%\n",
      "CPU: 54.7% | RAM: 82.8%\n",
      "CPU: 61.8% | RAM: 83.0%\n",
      "CPU: 52.2% | RAM: 82.7%\n",
      "CPU: 55.3% | RAM: 81.6%\n",
      "CPU: 53.3% | RAM: 82.1%\n",
      "CPU: 49.7% | RAM: 82.8%\n",
      "CPU: 63.0% | RAM: 82.6%\n",
      "CPU: 53.4% | RAM: 82.3%\n",
      "CPU: 50.7% | RAM: 82.4%\n",
      "CPU: 54.9% | RAM: 83.3%\n",
      "CPU: 57.1% | RAM: 82.7%\n",
      "CPU: 43.3% | RAM: 84.2%\n",
      "CPU: 58.4% | RAM: 80.9%\n",
      "CPU: 44.0% | RAM: 81.1%\n",
      "CPU: 46.0% | RAM: 81.9%\n",
      "CPU: 44.5% | RAM: 79.2%\n",
      "CPU: 45.1% | RAM: 77.1%\n",
      "CPU: 36.9% | RAM: 76.6%\n",
      "CPU: 35.5% | RAM: 76.7%\n",
      "CPU: 35.3% | RAM: 76.8%\n",
      "CPU: 33.7% | RAM: 77.1%\n",
      "CPU: 45.8% | RAM: 75.5%\n",
      "CPU: 37.3% | RAM: 75.5%\n",
      "CPU: 4.3% | RAM: 75.8%\n",
      "CPU: 59.8% | RAM: 77.0%\n",
      "CPU: 52.0% | RAM: 76.6%\n",
      "CPU: 35.7% | RAM: 76.0%\n",
      "CPU: 44.0% | RAM: 78.0%\n",
      "CPU: 38.6% | RAM: 78.1%\n",
      "CPU: 40.7% | RAM: 77.9%\n",
      "CPU: 39.0% | RAM: 77.8%\n",
      "CPU: 19.3% | RAM: 76.2%\n",
      "CPU: 60.5% | RAM: 76.5%\n",
      "CPU: 59.7% | RAM: 76.7%\n",
      "CPU: 52.3% | RAM: 78.2%\n",
      "CPU: 49.7% | RAM: 75.9%\n",
      "CPU: 40.2% | RAM: 75.8%\n",
      "CPU: 46.0% | RAM: 76.1%\n",
      "CPU: 0.4% | RAM: 76.9%\n",
      "CPU: 61.4% | RAM: 76.6%\n",
      "CPU: 49.0% | RAM: 76.0%\n",
      "CPU: 45.9% | RAM: 76.9%\n",
      "CPU: 41.5% | RAM: 77.5%\n",
      "CPU: 56.1% | RAM: 78.7%\n",
      "CPU: 56.4% | RAM: 81.6%\n",
      "CPU: 47.6% | RAM: 83.3%\n",
      "CPU: 56.5% | RAM: 80.1%\n",
      "CPU: 49.4% | RAM: 78.3%\n",
      "CPU: 51.8% | RAM: 80.6%\n",
      "CPU: 57.7% | RAM: 80.1%\n",
      "CPU: 48.3% | RAM: 79.2%\n",
      "CPU: 44.3% | RAM: 81.5%\n",
      "CPU: 54.3% | RAM: 81.7%\n",
      "CPU: 51.1% | RAM: 81.0%\n",
      "CPU: 43.8% | RAM: 81.5%\n",
      "CPU: 52.9% | RAM: 79.9%\n",
      "CPU: 45.4% | RAM: 80.4%\n",
      "CPU: 43.3% | RAM: 80.4%\n",
      "CPU: 49.5% | RAM: 80.1%\n",
      "CPU: 44.3% | RAM: 80.3%\n",
      "CPU: 49.8% | RAM: 80.3%\n",
      "CPU: 41.4% | RAM: 80.2%\n",
      "CPU: 41.8% | RAM: 80.4%\n",
      "CPU: 43.7% | RAM: 79.9%\n",
      "CPU: 46.6% | RAM: 79.1%\n",
      "CPU: 38.6% | RAM: 78.2%\n",
      "CPU: 47.4% | RAM: 77.7%\n",
      "CPU: 43.0% | RAM: 77.8%\n",
      "CPU: 42.7% | RAM: 79.4%\n",
      "CPU: 50.3% | RAM: 80.6%\n",
      "CPU: 45.3% | RAM: 79.8%\n",
      "CPU: 40.0% | RAM: 78.8%\n",
      "CPU: 38.0% | RAM: 79.3%\n",
      "CPU: 37.0% | RAM: 80.1%\n",
      "CPU: 45.5% | RAM: 79.8%\n",
      "CPU: 38.9% | RAM: 78.6%\n",
      "CPU: 38.7% | RAM: 78.2%\n",
      "CPU: 38.2% | RAM: 78.2%\n",
      "CPU: 36.8% | RAM: 78.8%\n",
      "CPU: 41.8% | RAM: 77.0%\n",
      "CPU: 41.5% | RAM: 78.9%\n",
      "CPU: 37.4% | RAM: 79.1%\n",
      "CPU: 43.0% | RAM: 79.3%\n",
      "CPU: 37.4% | RAM: 78.7%\n",
      "CPU: 42.9% | RAM: 79.9%\n",
      "CPU: 39.3% | RAM: 78.5%\n",
      "CPU: 34.6% | RAM: 79.2%\n",
      "CPU: 48.8% | RAM: 79.1%\n",
      "CPU: 41.2% | RAM: 78.9%\n",
      "CPU: 39.8% | RAM: 79.4%\n",
      "CPU: 39.2% | RAM: 79.5%\n",
      "CPU: 37.2% | RAM: 79.1%\n",
      "CPU: 38.5% | RAM: 78.9%\n",
      "CPU: 44.4% | RAM: 79.2%\n",
      "CPU: 34.5% | RAM: 81.1%\n",
      "CPU: 46.1% | RAM: 78.9%\n",
      "CPU: 38.6% | RAM: 79.1%\n",
      "CPU: 40.7% | RAM: 79.4%\n",
      "CPU: 38.2% | RAM: 80.0%\n",
      "CPU: 39.7% | RAM: 79.5%\n",
      "CPU: 45.1% | RAM: 79.4%\n",
      "CPU: 40.0% | RAM: 79.6%\n",
      "CPU: 37.3% | RAM: 80.3%\n",
      "CPU: 43.5% | RAM: 79.0%\n",
      "CPU: 46.8% | RAM: 80.2%\n",
      "CPU: 42.0% | RAM: 79.7%\n",
      "CPU: 38.0% | RAM: 79.0%\n",
      "CPU: 39.0% | RAM: 79.4%\n",
      "CPU: 40.5% | RAM: 79.5%\n",
      "CPU: 41.2% | RAM: 78.5%\n",
      "CPU: 36.5% | RAM: 78.5%\n",
      "CPU: 74.8% | RAM: 84.8%\n",
      "CPU: 52.0% | RAM: 81.7%\n",
      "CPU: 25.6% | RAM: 82.0%\n",
      "CPU: 44.5% | RAM: 82.1%\n",
      "CPU: 30.1% | RAM: 80.7%\n",
      "CPU: 30.9% | RAM: 80.8%\n",
      "CPU: 32.0% | RAM: 81.3%\n",
      "CPU: 26.7% | RAM: 82.3%\n",
      "CPU: 31.6% | RAM: 81.3%\n",
      "CPU: 31.1% | RAM: 80.6%\n",
      "CPU: 8.7% | RAM: 83.7%\n",
      "CPU: 2.2% | RAM: 85.3%\n",
      "CPU: 3.4% | RAM: 87.9%\n",
      "CPU: 3.6% | RAM: 88.8%\n",
      "CPU: 2.7% | RAM: 87.6%\n",
      "CPU: 3.3% | RAM: 87.7%\n",
      "CPU: 1.9% | RAM: 87.4%\n",
      "CPU: 2.6% | RAM: 84.7%\n",
      "CPU: 2.7% | RAM: 86.7%\n",
      "CPU: 3.2% | RAM: 87.7%\n",
      "CPU: 2.8% | RAM: 89.0%\n",
      "CPU: 3.3% | RAM: 91.7%\n",
      "CPU: 5.5% | RAM: 89.5%\n",
      "CPU: 3.0% | RAM: 90.0%\n",
      "CPU: 5.5% | RAM: 89.8%\n",
      "CPU: 5.5% | RAM: 92.6%\n",
      "CPU: 4.2% | RAM: 88.8%\n",
      "CPU: 2.6% | RAM: 88.6%\n",
      "CPU: 7.2% | RAM: 91.0%\n",
      "CPU: 5.2% | RAM: 94.1%\n",
      "CPU: 8.5% | RAM: 91.9%\n",
      "CPU: 4.0% | RAM: 93.6%\n",
      "CPU: 4.9% | RAM: 93.8%\n",
      "CPU: 4.0% | RAM: 91.9%\n",
      "CPU: 6.2% | RAM: 91.1%\n",
      "CPU: 2.1% | RAM: 91.0%\n",
      "CPU: 1.3% | RAM: 91.2%\n",
      "CPU: 2.4% | RAM: 89.5%\n",
      "CPU: 1.3% | RAM: 89.3%\n",
      "CPU: 2.2% | RAM: 85.2%\n",
      "CPU: 1.5% | RAM: 85.1%\n",
      "CPU: 1.9% | RAM: 85.2%\n",
      "CPU: 1.4% | RAM: 85.5%\n",
      "CPU: 3.1% | RAM: 85.5%\n",
      "CPU: 1.4% | RAM: 85.2%\n",
      "CPU: 1.8% | RAM: 84.9%\n",
      "CPU: 1.5% | RAM: 84.9%\n",
      "CPU: 1.4% | RAM: 84.9%\n",
      "CPU: 1.5% | RAM: 84.8%\n",
      "CPU: 2.8% | RAM: 84.2%\n",
      "CPU: 1.7% | RAM: 84.2%\n",
      "CPU: 1.5% | RAM: 84.3%\n",
      "CPU: 1.9% | RAM: 84.5%\n",
      "CPU: 1.8% | RAM: 84.5%\n",
      "CPU: 1.4% | RAM: 84.3%\n",
      "CPU: 1.9% | RAM: 84.7%\n",
      "CPU: 1.4% | RAM: 84.8%\n",
      "CPU: 1.6% | RAM: 85.1%\n",
      "CPU: 2.1% | RAM: 85.1%\n",
      "CPU: 1.4% | RAM: 85.0%\n",
      "CPU: 1.7% | RAM: 85.2%\n",
      "CPU: 1.6% | RAM: 85.2%\n",
      "CPU: 1.5% | RAM: 85.5%\n",
      "CPU: 2.4% | RAM: 85.1%\n",
      "CPU: 1.6% | RAM: 84.5%\n",
      "CPU: 1.8% | RAM: 84.5%\n",
      "CPU: 1.4% | RAM: 84.5%\n",
      "CPU: 1.5% | RAM: 84.9%\n",
      "CPU: 1.4% | RAM: 84.6%\n",
      "CPU: 2.4% | RAM: 84.7%\n",
      "CPU: 1.4% | RAM: 85.0%\n",
      "CPU: 1.9% | RAM: 85.0%\n",
      "CPU: 1.4% | RAM: 85.3%\n",
      "CPU: 1.6% | RAM: 85.3%\n",
      "CPU: 1.5% | RAM: 85.0%\n",
      "CPU: 1.6% | RAM: 84.9%\n",
      "CPU: 1.5% | RAM: 84.9%\n",
      "CPU: 1.9% | RAM: 85.1%\n",
      "CPU: 1.9% | RAM: 85.0%\n",
      "CPU: 2.2% | RAM: 84.7%\n",
      "CPU: 1.7% | RAM: 84.9%\n",
      "CPU: 1.2% | RAM: 84.9%\n",
      "CPU: 1.5% | RAM: 85.4%\n",
      "CPU: 1.9% | RAM: 85.3%\n",
      "CPU: 1.4% | RAM: 85.1%\n",
      "CPU: 2.3% | RAM: 86.4%\n",
      "CPU: 1.4% | RAM: 86.4%\n",
      "CPU: 1.2% | RAM: 86.9%\n",
      "CPU: 1.3% | RAM: 86.5%\n",
      "CPU: 1.5% | RAM: 86.4%\n",
      "CPU: 1.7% | RAM: 86.7%\n",
      "CPU: 1.7% | RAM: 86.6%\n",
      "CPU: 1.3% | RAM: 87.0%\n",
      "CPU: 2.3% | RAM: 86.6%\n",
      "CPU: 2.7% | RAM: 86.0%\n",
      "CPU: 0.0% | RAM: 85.9%\n",
      "CPU: 10.8% | RAM: 89.0%\n",
      "CPU: 5.0% | RAM: 85.5%\n",
      "CPU: 2.9% | RAM: 86.0%\n",
      "CPU: 2.1% | RAM: 85.9%\n",
      "CPU: 2.1% | RAM: 85.5%\n",
      "CPU: 2.6% | RAM: 85.5%\n",
      "CPU: 1.9% | RAM: 85.7%\n",
      "CPU: 2.1% | RAM: 85.5%\n",
      "CPU: 2.6% | RAM: 85.4%\n",
      "CPU: 1.8% | RAM: 85.3%\n",
      "CPU: 1.9% | RAM: 85.3%\n",
      "CPU: 2.3% | RAM: 85.3%\n",
      "CPU: 2.1% | RAM: 85.6%\n",
      "CPU: 1.8% | RAM: 85.8%\n",
      "CPU: 1.5% | RAM: 85.4%\n",
      "CPU: 1.7% | RAM: 85.5%\n",
      "CPU: 1.9% | RAM: 85.0%\n",
      "CPU: 1.8% | RAM: 84.9%\n",
      "CPU: 2.1% | RAM: 84.9%\n",
      "CPU: 1.5% | RAM: 84.9%\n",
      "CPU: 1.7% | RAM: 84.8%\n",
      "CPU: 1.4% | RAM: 84.8%\n",
      "CPU: 1.7% | RAM: 85.1%\n",
      "CPU: 1.2% | RAM: 85.3%\n",
      "CPU: 1.6% | RAM: 84.8%\n",
      "CPU: 1.6% | RAM: 84.7%\n",
      "CPU: 1.5% | RAM: 84.8%\n",
      "CPU: 1.6% | RAM: 84.7%\n",
      "CPU: 1.5% | RAM: 85.3%\n",
      "CPU: 1.7% | RAM: 85.5%\n",
      "CPU: 2.0% | RAM: 85.8%\n",
      "CPU: 2.5% | RAM: 85.1%\n",
      "CPU: 1.7% | RAM: 85.3%\n",
      "CPU: 1.4% | RAM: 85.6%\n",
      "CPU: 1.5% | RAM: 85.4%\n",
      "CPU: 1.5% | RAM: 85.4%\n",
      "CPU: 2.2% | RAM: 86.0%\n",
      "CPU: 1.8% | RAM: 85.0%\n",
      "CPU: 1.5% | RAM: 85.5%\n",
      "CPU: 1.4% | RAM: 85.3%\n",
      "CPU: 1.6% | RAM: 85.4%\n",
      "CPU: 1.1% | RAM: 85.4%\n",
      "CPU: 2.1% | RAM: 85.4%\n",
      "CPU: 1.9% | RAM: 85.7%\n",
      "CPU: 1.5% | RAM: 85.4%\n",
      "CPU: 1.6% | RAM: 85.4%\n",
      "CPU: 1.9% | RAM: 85.1%\n",
      "CPU: 1.5% | RAM: 85.1%\n",
      "CPU: 1.6% | RAM: 85.4%\n",
      "CPU: 1.7% | RAM: 85.4%\n",
      "CPU: 2.0% | RAM: 85.9%\n",
      "CPU: 1.4% | RAM: 86.0%\n",
      "CPU: 1.7% | RAM: 86.2%\n",
      "CPU: 1.0% | RAM: 86.5%\n",
      "CPU: 1.4% | RAM: 86.4%\n",
      "CPU: 2.1% | RAM: 86.1%\n",
      "CPU: 1.4% | RAM: 86.1%\n",
      "CPU: 1.8% | RAM: 85.9%\n",
      "CPU: 1.1% | RAM: 86.1%\n",
      "CPU: 0.1% | RAM: 86.4%\n",
      "CPU: 15.6% | RAM: 86.9%\n",
      "CPU: 2.2% | RAM: 85.9%\n",
      "CPU: 1.5% | RAM: 85.6%\n",
      "CPU: 1.6% | RAM: 85.1%\n",
      "CPU: 1.6% | RAM: 85.1%\n",
      "CPU: 2.0% | RAM: 84.9%\n",
      "CPU: 1.9% | RAM: 85.2%\n",
      "CPU: 1.0% | RAM: 84.8%\n",
      "CPU: 1.9% | RAM: 84.9%\n",
      "CPU: 1.5% | RAM: 84.9%\n",
      "CPU: 1.5% | RAM: 85.0%\n",
      "CPU: 1.3% | RAM: 85.4%\n",
      "CPU: 1.4% | RAM: 84.8%\n",
      "CPU: 1.5% | RAM: 84.6%\n",
      "CPU: 1.5% | RAM: 84.7%\n",
      "CPU: 1.7% | RAM: 84.6%\n",
      "CPU: 2.9% | RAM: 85.0%\n",
      "CPU: 1.7% | RAM: 84.4%\n",
      "CPU: 10.2% | RAM: 86.8%\n",
      "CPU: 2.5% | RAM: 85.1%\n",
      "CPU: 1.7% | RAM: 85.2%\n",
      "CPU: 1.2% | RAM: 85.5%\n",
      "CPU: 1.2% | RAM: 85.1%\n",
      "CPU: 1.6% | RAM: 85.1%\n",
      "CPU: 4.5% | RAM: 86.6%\n",
      "CPU: 6.1% | RAM: 85.0%\n",
      "CPU: 2.3% | RAM: 85.2%\n",
      "CPU: 1.4% | RAM: 85.1%\n",
      "CPU: 2.2% | RAM: 84.7%\n",
      "CPU: 1.7% | RAM: 84.5%\n",
      "CPU: 2.5% | RAM: 85.0%\n",
      "CPU: 1.7% | RAM: 84.8%\n",
      "CPU: 1.2% | RAM: 84.7%\n",
      "CPU: 1.8% | RAM: 84.6%\n",
      "CPU: 1.6% | RAM: 84.6%\n",
      "CPU: 2.1% | RAM: 84.8%\n",
      "CPU: 1.9% | RAM: 84.6%\n",
      "CPU: 1.2% | RAM: 84.5%\n",
      "CPU: 1.7% | RAM: 84.1%\n",
      "CPU: 1.6% | RAM: 84.1%\n",
      "CPU: 1.8% | RAM: 84.1%\n",
      "CPU: 1.6% | RAM: 84.5%\n",
      "CPU: 1.4% | RAM: 84.7%\n",
      "CPU: 1.8% | RAM: 84.5%\n",
      "CPU: 1.6% | RAM: 84.6%\n",
      "CPU: 1.6% | RAM: 84.5%\n",
      "CPU: 2.6% | RAM: 84.9%\n",
      "CPU: 1.5% | RAM: 85.3%\n",
      "CPU: 1.8% | RAM: 84.9%\n",
      "CPU: 1.8% | RAM: 84.9%\n",
      "CPU: 2.1% | RAM: 85.1%\n",
      "CPU: 1.4% | RAM: 85.1%\n",
      "CPU: 1.5% | RAM: 85.1%\n",
      "CPU: 1.7% | RAM: 84.9%\n",
      "CPU: 1.7% | RAM: 84.5%\n",
      "CPU: 1.8% | RAM: 84.4%\n",
      "CPU: 2.0% | RAM: 85.1%\n",
      "CPU: 1.1% | RAM: 84.5%\n",
      "CPU: 1.7% | RAM: 84.7%\n",
      "CPU: 1.5% | RAM: 84.8%\n",
      "CPU: 0.0% | RAM: 85.6%\n",
      "CPU: 12.2% | RAM: 91.2%\n",
      "CPU: 7.1% | RAM: 88.7%\n",
      "CPU: 3.1% | RAM: 90.8%\n",
      "CPU: 4.9% | RAM: 92.0%\n",
      "CPU: 4.1% | RAM: 89.8%\n",
      "CPU: 7.1% | RAM: 89.9%\n",
      "CPU: 13.2% | RAM: 83.4%\n",
      "CPU: 4.7% | RAM: 81.8%\n",
      "CPU: 3.2% | RAM: 80.3%\n",
      "CPU: 14.3% | RAM: 81.9%\n",
      "CPU: 4.4% | RAM: 84.1%\n",
      "CPU: 2.7% | RAM: 77.3%\n",
      "CPU: 2.0% | RAM: 77.4%\n",
      "CPU: 17.9% | RAM: 77.3%\n",
      "CPU: 10.9% | RAM: 79.5%\n",
      "CPU: 2.7% | RAM: 79.1%\n",
      "CPU: 5.3% | RAM: 80.3%\n",
      "CPU: 6.1% | RAM: 82.8%\n",
      "CPU: 3.2% | RAM: 80.8%\n",
      "CPU: 6.6% | RAM: 85.8%\n",
      "CPU: 5.2% | RAM: 83.5%\n",
      "CPU: 5.4% | RAM: 83.3%\n",
      "CPU: 2.7% | RAM: 82.5%\n",
      "CPU: 3.7% | RAM: 81.2%\n",
      "CPU: 2.7% | RAM: 81.6%\n",
      "CPU: 3.3% | RAM: 81.8%\n",
      "CPU: 3.0% | RAM: 78.9%\n",
      "CPU: 3.5% | RAM: 79.6%\n",
      "CPU: 2.8% | RAM: 78.9%\n",
      "CPU: 3.4% | RAM: 82.2%\n",
      "CPU: 2.9% | RAM: 82.6%\n",
      "CPU: 4.1% | RAM: 82.6%\n",
      "CPU: 5.3% | RAM: 81.3%\n",
      "CPU: 4.0% | RAM: 80.9%\n",
      "CPU: 4.4% | RAM: 81.3%\n",
      "CPU: 5.6% | RAM: 84.7%\n",
      "CPU: 12.9% | RAM: 80.3%\n",
      "CPU: 5.2% | RAM: 80.4%\n",
      "CPU: 3.2% | RAM: 79.6%\n",
      "CPU: 2.3% | RAM: 78.4%\n",
      "CPU: 3.0% | RAM: 78.6%\n",
      "CPU: 2.9% | RAM: 78.7%\n",
      "CPU: 3.7% | RAM: 77.6%\n",
      "CPU: 3.5% | RAM: 78.6%\n",
      "CPU: 2.6% | RAM: 78.4%\n",
      "CPU: 5.1% | RAM: 82.1%\n",
      "CPU: 3.3% | RAM: 80.4%\n",
      "CPU: 2.7% | RAM: 80.4%\n",
      "CPU: 2.1% | RAM: 79.3%\n",
      "CPU: 2.8% | RAM: 78.9%\n",
      "CPU: 2.0% | RAM: 78.6%\n",
      "CPU: 2.4% | RAM: 78.5%\n",
      "CPU: 1.5% | RAM: 78.9%\n",
      "CPU: 3.2% | RAM: 80.8%\n",
      "CPU: 5.3% | RAM: 82.1%\n",
      "CPU: 3.3% | RAM: 79.8%\n",
      "CPU: 3.2% | RAM: 77.7%\n",
      "CPU: 2.6% | RAM: 77.6%\n",
      "CPU: 2.6% | RAM: 77.6%\n",
      "CPU: 2.5% | RAM: 78.7%\n",
      "CPU: 3.6% | RAM: 78.5%\n",
      "CPU: 5.7% | RAM: 79.7%\n",
      "CPU: 6.2% | RAM: 79.9%\n",
      "CPU: 2.7% | RAM: 74.6%\n",
      "CPU: 2.4% | RAM: 74.9%\n",
      "CPU: 4.0% | RAM: 79.8%\n",
      "CPU: 3.3% | RAM: 73.7%\n",
      "CPU: 2.3% | RAM: 74.6%\n",
      "CPU: 4.7% | RAM: 77.9%\n",
      "CPU: 6.9% | RAM: 74.4%\n",
      "CPU: 2.9% | RAM: 74.1%\n",
      "CPU: 2.8% | RAM: 74.4%\n",
      "CPU: 3.5% | RAM: 76.0%\n",
      "CPU: 2.7% | RAM: 74.4%\n",
      "CPU: 2.2% | RAM: 73.9%\n",
      "CPU: 2.8% | RAM: 74.3%\n",
      "CPU: 4.1% | RAM: 73.7%\n",
      "CPU: 5.1% | RAM: 76.2%\n",
      "CPU: 15.7% | RAM: 79.3%\n",
      "CPU: 11.7% | RAM: 78.9%\n",
      "CPU: 3.0% | RAM: 78.0%\n",
      "CPU: 4.0% | RAM: 76.7%\n",
      "CPU: 5.2% | RAM: 76.7%\n",
      "CPU: 6.4% | RAM: 79.9%\n",
      "CPU: 3.1% | RAM: 79.2%\n",
      "CPU: 2.8% | RAM: 79.7%\n",
      "CPU: 6.8% | RAM: 82.6%\n",
      "CPU: 3.6% | RAM: 81.4%\n",
      "CPU: 3.1% | RAM: 81.4%\n",
      "CPU: 3.8% | RAM: 81.8%\n",
      "CPU: 3.5% | RAM: 82.5%\n",
      "CPU: 3.2% | RAM: 82.0%\n",
      "CPU: 3.8% | RAM: 84.0%\n",
      "CPU: 3.2% | RAM: 81.3%\n",
      "CPU: 3.2% | RAM: 81.3%\n",
      "CPU: 3.1% | RAM: 81.8%\n",
      "CPU: 3.6% | RAM: 82.5%\n",
      "CPU: 3.0% | RAM: 81.6%\n",
      "CPU: 4.7% | RAM: 84.4%\n",
      "CPU: 3.2% | RAM: 81.7%\n",
      "CPU: 3.2% | RAM: 81.0%\n",
      "CPU: 3.7% | RAM: 82.0%\n",
      "CPU: 4.8% | RAM: 83.4%\n",
      "CPU: 3.5% | RAM: 82.9%\n",
      "CPU: 3.1% | RAM: 81.8%\n",
      "CPU: 4.9% | RAM: 81.7%\n",
      "CPU: 6.1% | RAM: 81.8%\n",
      "CPU: 3.6% | RAM: 82.7%\n",
      "CPU: 3.5% | RAM: 81.7%\n",
      "CPU: 2.0% | RAM: 81.0%\n",
      "CPU: 2.5% | RAM: 82.2%\n",
      "CPU: 7.0% | RAM: 81.4%\n",
      "CPU: 3.2% | RAM: 82.6%\n",
      "CPU: 3.1% | RAM: 81.9%\n",
      "CPU: 3.3% | RAM: 82.4%\n",
      "CPU: 2.5% | RAM: 81.3%\n",
      "CPU: 4.3% | RAM: 79.3%\n",
      "CPU: 3.8% | RAM: 80.3%\n",
      "CPU: 6.5% | RAM: 84.1%\n",
      "CPU: 3.4% | RAM: 79.0%\n",
      "CPU: 7.5% | RAM: 85.0%\n",
      "CPU: 5.3% | RAM: 78.4%\n",
      "CPU: 3.5% | RAM: 78.5%\n",
      "CPU: 3.9% | RAM: 79.2%\n",
      "CPU: 2.7% | RAM: 79.0%\n",
      "CPU: 3.8% | RAM: 78.5%\n",
      "CPU: 3.4% | RAM: 78.7%\n",
      "CPU: 5.0% | RAM: 78.4%\n",
      "CPU: 3.9% | RAM: 80.9%\n",
      "CPU: 5.6% | RAM: 80.0%\n",
      "CPU: 3.3% | RAM: 79.6%\n",
      "CPU: 2.5% | RAM: 78.7%\n",
      "CPU: 4.6% | RAM: 78.1%\n",
      "CPU: 3.6% | RAM: 79.4%\n",
      "CPU: 4.4% | RAM: 81.0%\n",
      "CPU: 2.8% | RAM: 80.8%\n",
      "CPU: 3.9% | RAM: 79.3%\n",
      "CPU: 6.4% | RAM: 80.3%\n",
      "CPU: 3.4% | RAM: 79.3%\n",
      "CPU: 4.1% | RAM: 80.8%\n",
      "CPU: 6.2% | RAM: 80.0%\n",
      "CPU: 3.2% | RAM: 80.6%\n",
      "CPU: 3.4% | RAM: 79.5%\n",
      "CPU: 5.1% | RAM: 79.8%\n",
      "CPU: 2.7% | RAM: 80.3%\n",
      "CPU: 3.1% | RAM: 80.1%\n",
      "CPU: 4.6% | RAM: 80.2%\n",
      "CPU: 3.9% | RAM: 79.6%\n",
      "CPU: 2.8% | RAM: 80.0%\n",
      "CPU: 2.9% | RAM: 80.4%\n",
      "CPU: 4.9% | RAM: 80.8%\n",
      "CPU: 3.2% | RAM: 80.3%\n",
      "CPU: 3.0% | RAM: 79.3%\n",
      "CPU: 3.1% | RAM: 79.4%\n",
      "CPU: 8.8% | RAM: 78.8%\n",
      "CPU: 3.4% | RAM: 79.7%\n",
      "CPU: 6.3% | RAM: 79.3%\n",
      "CPU: 8.7% | RAM: 78.8%\n",
      "CPU: 17.9% | RAM: 78.4%\n",
      "CPU: 13.1% | RAM: 79.1%\n",
      "CPU: 3.8% | RAM: 80.3%\n",
      "CPU: 4.2% | RAM: 80.5%\n",
      "CPU: 5.6% | RAM: 80.2%\n",
      "CPU: 3.7% | RAM: 80.6%\n",
      "CPU: 6.4% | RAM: 80.5%\n",
      "CPU: 4.6% | RAM: 80.4%\n",
      "CPU: 3.3% | RAM: 79.5%\n",
      "CPU: 5.7% | RAM: 78.4%\n",
      "CPU: 3.1% | RAM: 78.4%\n",
      "CPU: 3.3% | RAM: 79.0%\n",
      "CPU: 4.2% | RAM: 79.6%\n",
      "CPU: 6.6% | RAM: 78.9%\n",
      "CPU: 3.5% | RAM: 78.0%\n",
      "CPU: 6.7% | RAM: 78.7%\n",
      "CPU: 7.2% | RAM: 76.8%\n",
      "CPU: 4.0% | RAM: 77.8%\n",
      "CPU: 5.5% | RAM: 77.7%\n",
      "CPU: 5.3% | RAM: 77.8%\n",
      "CPU: 3.2% | RAM: 76.9%\n",
      "CPU: 3.4% | RAM: 78.3%\n",
      "CPU: 3.0% | RAM: 79.4%\n",
      "CPU: 3.0% | RAM: 79.7%\n",
      "CPU: 3.8% | RAM: 79.7%\n",
      "CPU: 9.4% | RAM: 79.3%\n",
      "CPU: 10.4% | RAM: 79.0%\n",
      "CPU: 3.0% | RAM: 79.7%\n",
      "CPU: 5.4% | RAM: 78.8%\n",
      "CPU: 4.1% | RAM: 78.9%\n",
      "CPU: 2.8% | RAM: 78.9%\n",
      "CPU: 3.0% | RAM: 78.8%\n",
      "CPU: 7.7% | RAM: 79.0%\n",
      "CPU: 4.5% | RAM: 77.9%\n",
      "CPU: 3.4% | RAM: 77.8%\n",
      "CPU: 3.3% | RAM: 77.9%\n",
      "CPU: 5.2% | RAM: 78.0%\n",
      "CPU: 6.0% | RAM: 78.4%\n",
      "CPU: 3.1% | RAM: 78.2%\n",
      "CPU: 3.5% | RAM: 78.6%\n",
      "CPU: 4.2% | RAM: 78.6%\n",
      "CPU: 3.0% | RAM: 78.4%\n",
      "CPU: 3.6% | RAM: 79.9%\n",
      "CPU: 3.8% | RAM: 78.9%\n",
      "CPU: 3.6% | RAM: 78.4%\n",
      "CPU: 4.3% | RAM: 81.8%\n",
      "CPU: 4.0% | RAM: 81.5%\n",
      "CPU: 3.4% | RAM: 80.0%\n",
      "CPU: 4.3% | RAM: 79.5%\n",
      "CPU: 4.4% | RAM: 79.7%\n",
      "CPU: 3.3% | RAM: 79.9%\n",
      "CPU: 3.7% | RAM: 79.3%\n",
      "CPU: 3.3% | RAM: 80.2%\n",
      "CPU: 3.8% | RAM: 79.3%\n",
      "CPU: 3.4% | RAM: 76.7%\n",
      "CPU: 3.2% | RAM: 77.0%\n",
      "CPU: 3.9% | RAM: 76.3%\n",
      "CPU: 3.0% | RAM: 76.3%\n",
      "CPU: 3.4% | RAM: 76.5%\n",
      "CPU: 5.3% | RAM: 76.5%\n",
      "CPU: 3.3% | RAM: 75.8%\n",
      "CPU: 3.1% | RAM: 75.6%\n",
      "CPU: 2.5% | RAM: 75.5%\n",
      "CPU: 3.2% | RAM: 75.2%\n",
      "CPU: 3.0% | RAM: 75.0%\n",
      "CPU: 3.1% | RAM: 75.5%\n",
      "CPU: 2.8% | RAM: 75.8%\n",
      "CPU: 3.3% | RAM: 76.1%\n",
      "CPU: 3.0% | RAM: 76.3%\n",
      "CPU: 2.8% | RAM: 75.6%\n",
      "CPU: 2.3% | RAM: 75.4%\n",
      "CPU: 2.8% | RAM: 75.5%\n",
      "CPU: 5.3% | RAM: 76.7%\n",
      "CPU: 2.8% | RAM: 76.7%\n",
      "CPU: 3.7% | RAM: 75.7%\n",
      "CPU: 3.0% | RAM: 75.9%\n",
      "CPU: 13.7% | RAM: 78.4%\n",
      "CPU: 13.5% | RAM: 77.0%\n",
      "CPU: 5.9% | RAM: 75.8%\n",
      "CPU: 2.8% | RAM: 76.2%\n",
      "CPU: 3.2% | RAM: 76.2%\n",
      "CPU: 2.9% | RAM: 76.0%\n",
      "CPU: 3.6% | RAM: 78.1%\n",
      "CPU: 4.4% | RAM: 79.6%\n",
      "CPU: 4.5% | RAM: 80.8%\n",
      "CPU: 4.1% | RAM: 81.2%\n",
      "CPU: 4.2% | RAM: 82.2%\n",
      "CPU: 4.8% | RAM: 82.3%\n",
      "CPU: 4.9% | RAM: 81.8%\n",
      "CPU: 5.4% | RAM: 82.0%\n",
      "CPU: 7.8% | RAM: 84.8%\n",
      "CPU: 4.2% | RAM: 83.5%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import threading\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "#                        CONFIGURATION GLOBALE\n",
    "# --------------------------------------------------------------------\n",
    "IMG_SIZE = 224\n",
    "PATCH_SIZE = 16\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 40\n",
    "\n",
    "PROJECTION_DIM = 256\n",
    "NUM_HEADS = 4\n",
    "TRANSFORMER_LAYERS = 6\n",
    "MLP_DROPOUT = 0.2\n",
    "ATTENTION_DROPOUT = 0.1\n",
    "\n",
    "# Chemins des données\n",
    "train_dir = r\"C:\\Users\\Probook\\OneDrive\\Bureau\\DeepLearningTask\\DeepLearningTask-1\\DeepLearning 24-25\\Train\"\n",
    "test_dir = r\"C:\\Users\\Probook\\OneDrive\\Bureau\\DeepLearningTask\\DeepLearningTask-1\\DeepLearning 24-25\\Test\"\n",
    "\n",
    "NUM_CLASSES = 7  # ex: 7 classes\n",
    "\n",
    "# Data Augmentation\n",
    "ROTATION_RANGE    = 20\n",
    "ZOOM_RANGE        = (0.7, 1.0)\n",
    "WIDTH_SHIFT       = 0.3\n",
    "HEIGHT_SHIFT      = 0.3\n",
    "BRIGHTNESS_RANGE  = (0.8, 1.2)\n",
    "VALIDATION_SPLIT  = 0.2\n",
    "\n",
    "# Class weighting (exemple)\n",
    "class_counts = np.array([346, 353, 346, 337, 355, 340, 315], dtype=np.float32)\n",
    "class_weights = 1.0 / np.sqrt(class_counts)\n",
    "class_weights_dict = {i: w for i, w in enumerate(class_weights)}\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "#                     DATA AUGMENTATION & CHARGEMENT\n",
    "# --------------------------------------------------------------------\n",
    "train_data_gen = ImageDataGenerator(\n",
    "    rescale=1.0/255,\n",
    "    rotation_range=ROTATION_RANGE,\n",
    "    zoom_range=ZOOM_RANGE,\n",
    "    width_shift_range=WIDTH_SHIFT,\n",
    "    height_shift_range=HEIGHT_SHIFT,\n",
    "    brightness_range=BRIGHTNESS_RANGE,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    validation_split=VALIDATION_SPLIT\n",
    ")\n",
    "test_data_gen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "train_data = train_data_gen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_data = train_data_gen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_data = test_data_gen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(\"\\nClasses détectées (train) :\", train_data.class_indices)\n",
    "print(\"Nombre de classes (train) :\", train_data.num_classes)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "#              COUCHE CUSTOM POUR LE CLS TOKEN\n",
    "# --------------------------------------------------------------------\n",
    "class CLSTokenLayer(layers.Layer):\n",
    "    def __init__(self, projection_dim, **kwargs):\n",
    "        super(CLSTokenLayer, self).__init__(**kwargs)\n",
    "        self.projection_dim = projection_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.cls_token = self.add_weight(\n",
    "            name=\"cls_token\",\n",
    "            shape=(1, 1, self.projection_dim),\n",
    "            initializer=tf.keras.initializers.RandomNormal(stddev=0.02),\n",
    "            trainable=True\n",
    "        )\n",
    "        super(CLSTokenLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        cls_tokens = tf.tile(self.cls_token, [batch_size, 1, 1])\n",
    "        return tf.concat([cls_tokens, x], axis=1)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CLSTokenLayer, self).get_config()\n",
    "        config.update({\"projection_dim\": self.projection_dim})\n",
    "        return config\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "#               ARCHITECTURE DU VISION TRANSFORMER\n",
    "# --------------------------------------------------------------------\n",
    "def build_vit_model(\n",
    "    image_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    projection_dim=PROJECTION_DIM,\n",
    "    transformer_layers=TRANSFORMER_LAYERS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    mlp_dropout=MLP_DROPOUT,\n",
    "    attention_dropout=ATTENTION_DROPOUT\n",
    "):\n",
    "    num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "    inputs = layers.Input(shape=(image_size, image_size, 3))\n",
    "\n",
    "    # Patch Embedding\n",
    "    x = layers.Conv2D(\n",
    "        filters=projection_dim,\n",
    "        kernel_size=patch_size,\n",
    "        strides=patch_size,\n",
    "        padding='valid'\n",
    "    )(inputs)\n",
    "    x = layers.Reshape((num_patches, projection_dim))(x)\n",
    "\n",
    "    # CLS Token\n",
    "    x = CLSTokenLayer(projection_dim)(x)\n",
    "\n",
    "    # Position Embedding\n",
    "    positions = layers.Embedding(\n",
    "        input_dim=num_patches + 1,\n",
    "        output_dim=projection_dim\n",
    "    )(tf.range(start=0, limit=num_patches + 1))\n",
    "    positions = tf.expand_dims(positions, axis=0)\n",
    "    x = x + positions\n",
    "\n",
    "    # Dropout initial\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "\n",
    "    # Transformer blocks\n",
    "    for _ in range(transformer_layers):\n",
    "        # a) LayerNorm + Self-Attention\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        attn_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=projection_dim // num_heads,\n",
    "            dropout=attention_dropout\n",
    "        )(x1, x1)\n",
    "        x = x + attn_output\n",
    "\n",
    "        # b) MLP\n",
    "        x2 = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        x2 = layers.Dense(projection_dim * 2, activation='gelu')(x2)\n",
    "        x2 = layers.Dropout(mlp_dropout)(x2)\n",
    "        x2 = layers.Dense(projection_dim)(x2)\n",
    "        x = x + x2\n",
    "\n",
    "    # Classification : token CLS\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    cls_token_final = layers.Lambda(lambda t: t[:, 0])(x)\n",
    "\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(cls_token_final)\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "model = build_vit_model()\n",
    "model.summary()\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "#      COMPILATION AVEC COSINE DECAY + ADAMW + CLIPNORM\n",
    "# --------------------------------------------------------------------\n",
    "steps_per_epoch = len(train_data)\n",
    "total_steps = steps_per_epoch * EPOCHS\n",
    "initial_lr = 3e-4\n",
    "\n",
    "# Cosine Decay\n",
    "lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=initial_lr,\n",
    "    decay_steps=total_steps,\n",
    "    alpha=0.0\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.experimental.AdamW(\n",
    "    learning_rate=lr_schedule,\n",
    "    weight_decay=0.03,\n",
    "    clipnorm=1.0\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "#                    CALLBACKS & SURVEILLANCE CPU\n",
    "# --------------------------------------------------------------------import os\n",
    "import time\n",
    "import threading\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "#                        CONFIGURATION GLOBALE\n",
    "# --------------------------------------------------------------------\n",
    "IMG_SIZE = 224\n",
    "PATCH_SIZE = 16\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 40\n",
    "\n",
    "PROJECTION_DIM = 256\n",
    "NUM_HEADS = 4\n",
    "TRANSFORMER_LAYERS = 6\n",
    "MLP_DROPOUT = 0.2\n",
    "ATTENTION_DROPOUT = 0.1\n",
    "\n",
    "# Chemins des données\n",
    "train_dir = r\"C:\\Users\\Probook\\OneDrive\\Bureau\\DeepLearningTask\\DeepLearningTask-1\\DeepLearning 24-25\\Train\"\n",
    "test_dir = r\"C:\\Users\\Probook\\OneDrive\\Bureau\\DeepLearningTask\\DeepLearningTask-1\\DeepLearning 24-25\\Test\"\n",
    "\n",
    "NUM_CLASSES = 7  # ex: 7 classes\n",
    "\n",
    "# Data Augmentation\n",
    "ROTATION_RANGE    = 20\n",
    "ZOOM_RANGE        = (0.7, 1.0)\n",
    "WIDTH_SHIFT       = 0.3\n",
    "HEIGHT_SHIFT      = 0.3\n",
    "BRIGHTNESS_RANGE  = (0.8, 1.2)\n",
    "VALIDATION_SPLIT  = 0.2\n",
    "\n",
    "# Class weighting (exemple)\n",
    "class_counts = np.array([346, 353, 346, 337, 355, 340, 315], dtype=np.float32)\n",
    "class_weights = 1.0 / np.sqrt(class_counts)\n",
    "class_weights_dict = {i: w for i, w in enumerate(class_weights)}\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "#                     DATA AUGMENTATION & CHARGEMENT\n",
    "# --------------------------------------------------------------------\n",
    "train_data_gen = ImageDataGenerator(\n",
    "    rescale=1.0/255,\n",
    "    rotation_range=ROTATION_RANGE,\n",
    "    zoom_range=ZOOM_RANGE,\n",
    "    width_shift_range=WIDTH_SHIFT,\n",
    "    height_shift_range=HEIGHT_SHIFT,\n",
    "    brightness_range=BRIGHTNESS_RANGE,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    validation_split=VALIDATION_SPLIT\n",
    ")\n",
    "test_data_gen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "train_data = train_data_gen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_data = train_data_gen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_data = test_data_gen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(\"\\nClasses détectées (train) :\", train_data.class_indices)\n",
    "print(\"Nombre de classes (train) :\", train_data.num_classes)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "#              COUCHE CUSTOM POUR LE CLS TOKEN\n",
    "# --------------------------------------------------------------------\n",
    "class CLSTokenLayer(layers.Layer):\n",
    "    def __init__(self, projection_dim, **kwargs):\n",
    "        super(CLSTokenLayer, self).__init__(**kwargs)\n",
    "        self.projection_dim = projection_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.cls_token = self.add_weight(\n",
    "            name=\"cls_token\",\n",
    "            shape=(1, 1, self.projection_dim),\n",
    "            initializer=tf.keras.initializers.RandomNormal(stddev=0.02),\n",
    "            trainable=True\n",
    "        )\n",
    "        super(CLSTokenLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        cls_tokens = tf.tile(self.cls_token, [batch_size, 1, 1])\n",
    "        return tf.concat([cls_tokens, x], axis=1)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CLSTokenLayer, self).get_config()\n",
    "        config.update({\"projection_dim\": self.projection_dim})\n",
    "        return config\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "#               ARCHITECTURE DU VISION TRANSFORMER\n",
    "# --------------------------------------------------------------------\n",
    "def build_vit_model(\n",
    "    image_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    projection_dim=PROJECTION_DIM,\n",
    "    transformer_layers=TRANSFORMER_LAYERS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    mlp_dropout=MLP_DROPOUT,\n",
    "    attention_dropout=ATTENTION_DROPOUT\n",
    "):\n",
    "    num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "    inputs = layers.Input(shape=(image_size, image_size, 3))\n",
    "\n",
    "    # Patch Embedding\n",
    "    x = layers.Conv2D(\n",
    "        filters=projection_dim,\n",
    "        kernel_size=patch_size,\n",
    "        strides=patch_size,\n",
    "        padding='valid'\n",
    "    )(inputs)\n",
    "    x = layers.Reshape((num_patches, projection_dim))(x)\n",
    "\n",
    "    # CLS Token\n",
    "    x = CLSTokenLayer(projection_dim)(x)\n",
    "\n",
    "    # Position Embedding\n",
    "    positions = layers.Embedding(\n",
    "        input_dim=num_patches + 1,\n",
    "        output_dim=projection_dim\n",
    "    )(tf.range(start=0, limit=num_patches + 1))\n",
    "    positions = tf.expand_dims(positions, axis=0)\n",
    "    x = x + positions\n",
    "\n",
    "    # Dropout initial\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "\n",
    "    # Transformer blocks\n",
    "    for _ in range(transformer_layers):\n",
    "        # a) LayerNorm + Self-Attention\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        attn_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=projection_dim // num_heads,\n",
    "            dropout=attention_dropout\n",
    "        )(x1, x1)\n",
    "        x = x + attn_output\n",
    "\n",
    "        # b) MLP\n",
    "        x2 = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        x2 = layers.Dense(projection_dim * 2, activation='gelu')(x2)\n",
    "        x2 = layers.Dropout(mlp_dropout)(x2)\n",
    "        x2 = layers.Dense(projection_dim)(x2)\n",
    "        x = x + x2\n",
    "\n",
    "    # Classification : token CLS\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    cls_token_final = layers.Lambda(lambda t: t[:, 0])(x)\n",
    "\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(cls_token_final)\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "model = build_vit_model\n",
    "def monitor_cpu():\n",
    "    while True:\n",
    "        usage = psutil.cpu_percent()\n",
    "        mem = psutil.virtual_memory().percent\n",
    "        print(f\"CPU: {usage}% | RAM: {mem}%\")\n",
    "        time.sleep(60)\n",
    "\n",
    "threading.Thread(target=monitor_cpu, daemon=True).start()\n",
    "\n",
    "my_callbacks = [\n",
    "    callbacks.EarlyStopping(patience=10, restore_best_weights=True, monitor='val_accuracy'),\n",
    "    callbacks.ModelCheckpoint('best_vit.h5', save_best_only=True, monitor='val_accuracy')\n",
    "]\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "#                       ENTRAÎNEMENT\n",
    "# --------------------------------------------------------------------\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=len(val_data),\n",
    "    class_weight=class_weights_dict, \n",
    "    callbacks=my_callbacks,\n",
    "    verbose=1  # pour voir val_loss, val_accuracy, etc. à chaque epoch\n",
    ")\n",
    "\n",
    "best_val_acc = max(history.history['val_accuracy'])\n",
    "print(f\"\\nMeilleure val_accuracy pendant l'entraînement : {best_val_acc:.2%}\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "#             RÉCUPÉRER LE MEILLEUR MODÈLE & ÉVALUER\n",
    "# --------------------------------------------------------------------\n",
    "print(\"\\nChargement des meilleurs poids (best_vit.h5) ...\")\n",
    "model.load_weights('best_vit.h5')\n",
    "\n",
    "print(\"\\nÉvaluation finale sur la Validation :\")\n",
    "val_loss, val_acc = model.evaluate(val_data)\n",
    "print(f\"Val Loss: {val_loss:.4f} | Val Accuracy: {val_acc:.2%}\")\n",
    "\n",
    "print(\"\\nÉvaluation finale sur le Test :\")\n",
    "test_loss, test_acc = model.evaluate(test_data)\n",
    "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.2%}\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "#                       GRAPHIQUES\n",
    "# --------------------------------------------------------------------\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train acc')\n",
    "plt.plot(history.history['val_accuracy'], label='Val acc')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train loss')\n",
    "plt.plot(history.history['val_loss'], label='Val loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "#       EXEMPLES DE PRÉDICTIONS SUR VAL_DATA & TEST_DATA\n",
    "# --------------------------------------------------------------------\n",
    "inv_map_val = {v: k for k, v in val_data.class_indices.items()}\n",
    "print(\"\\nExemples de prédictions sur la Validation :\")\n",
    "for images, labels in val_data:\n",
    "    preds = model.predict(images)\n",
    "    for i in range(min(3, len(images))):\n",
    "        true_cls = inv_map_val[np.argmax(labels[i])]\n",
    "        pred_cls = inv_map_val[np.argmax(preds[i])]\n",
    "        print(f\"Prédiction: {pred_cls} | Vérité: {true_cls}\")\n",
    "    break\n",
    "\n",
    "inv_map_test = {v: k for k, v in test_data.class_indices.items()}\n",
    "print(\"\\nExemples de prédictions sur le Test :\")\n",
    "for images, labels in test_data:\n",
    "    preds = model.predict(images)\n",
    "    for i in range(min(3, len(images))):\n",
    "        true_cls = inv_map_test[np.argmax(labels[i])]\n",
    "        pred_cls = inv_map_test[np.argmax(preds[i])]\n",
    "        print(f\"Prédiction: {pred_cls} | Vérité: {true_cls}\")\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
